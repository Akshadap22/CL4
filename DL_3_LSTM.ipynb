{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEvij3xJLm_D",
        "outputId": "a80ef0d5-6aed-48ba-a0ed-2d1e5ea05aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 149s 184ms/step - loss: 0.4033 - accuracy: 0.8123 - val_loss: 0.3379 - val_accuracy: 0.8481\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 143s 182ms/step - loss: 0.2620 - accuracy: 0.8947 - val_loss: 0.4075 - val_accuracy: 0.8408\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 144s 184ms/step - loss: 0.2003 - accuracy: 0.9217 - val_loss: 0.3908 - val_accuracy: 0.8407\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 143s 183ms/step - loss: 0.1486 - accuracy: 0.9455 - val_loss: 0.4801 - val_accuracy: 0.8362\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 148s 189ms/step - loss: 0.1167 - accuracy: 0.9572 - val_loss: 0.4601 - val_accuracy: 0.8386\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 0.4601 - accuracy: 0.8386\n",
            "Test score:0.4601403772830963\n",
            "Test accuracy: 0.8386399745941162\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # Pad sequences to have a consistent length for the input to the RNN\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(f'Test score:{score}')\n",
        "print(f'Test accuracy: {acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O069kL6eLpnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}